import chromadb
from chromadb.utils import embedding_functions
import ollama
import os

ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")

print(f"DEBUG: –ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ Ollama –ø–æ –∞–¥—Ä–µ—Å—É: {ollama_host}")

client = ollama.Client(host=ollama_host)

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
DB_PATH = "./chroma_db_data"
COLLECTION_NAME = "labor_code"
MODEL_NAME = "gemma3:4b"

# 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–∞–∑–µ –î–∞–Ω–Ω—ã—Ö
client = chromadb.PersistentClient(path=DB_PATH)
emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
collection = client.get_collection(name=COLLECTION_NAME, embedding_function=emb_fn)


def get_context(query, n_results=3):
    """
    –ò—â–µ—Ç –≤ –±–∞–∑–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫–ª–µ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞.
    """
    results = collection.query(
        query_texts=[query],
        n_results=n_results
    )

    context_parts = []
    sources = []

    # –†–∞–∑–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if results['documents']:
        for i in range(len(results['documents'][0])):
            meta = results['metadatas'][0][i]

            # –í–ê–ñ–ù–û: –ú—ã –±–µ—Ä–µ–º 'original_full_text', –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö!
            # –≠—Ç–æ –¥–∞—Å—Ç –º–æ–¥–µ–ª–∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏, –∞ –Ω–µ –æ–±—Ä—ã–≤–æ–∫.
            full_text = meta.get('original_full_text', results['documents'][0][i])

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –±–ª–æ–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            source_info = f"–°—Ç–∞—Ç—å—è {meta['article_number']}: {meta['title']}"
            context_part = f"–ò–°–¢–û–ß–ù–ò–ö: {source_info}\n–¢–ï–ö–°–¢:\n{full_text}"

            context_parts.append(context_part)
            sources.append(source_info)

    return "\n\n---\n\n".join(context_parts), sources


def ask_labor_code(question):
    print(f"\nü§ñ –î—É–º–∞—é –Ω–∞–¥ –≤–æ–ø—Ä–æ—Å–æ–º: '{question}'...")

    # 1. Retrieval (–ü–æ–∏—Å–∫)
    context_text, sources = get_context(question)

    if not context_text:
        return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.", []

    # 2. Augmented Generation (–ü—Ä–æ–º–ø—Ç)
    prompt = f"""
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –¢—Ä—É–¥–æ–≤–æ–º—É –ö–æ–¥–µ–∫—Å—É –†–§.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ "–ö–û–ù–¢–ï–ö–°–¢". –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∑–∞–∫–æ–Ω—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ.
2. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π. –ù–∞–ø—Ä–∏–º–µ—Ä: "–°–æ–≥–ª–∞—Å–Ω–æ —Å—Ç. 261 –¢–ö –†–§...".
3. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ –Ω–∞–ø–∏—à–∏: "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–æ–º".
4. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º, —á–µ—Ç–∫–∏–º –∏ –≤–µ–∂–ª–∏–≤—ã–º.

–ö–û–ù–¢–ï–ö–°–¢:
{context_text}

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{question}
"""

    # 3. Generation (–í—ã–∑–æ–≤ Ollama)
    response = ollama.chat(model=MODEL_NAME, messages=[
        {
            'role': 'user',
            'content': prompt,
        },
    ])

    return response['message']['content'], sources


# --- –ó–ê–ü–£–°–ö ---
if __name__ == "__main__":
    while True:
        user_query = input("\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ")
        if user_query.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
            break

        answer, found_sources = ask_labor_code(user_query)

        print("\n" + "=" * 50)
        print("–û–¢–í–ï–¢ –Æ–†–ò–°–¢–ê:")
        print(answer)
        print("=" * 50)
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
        for s in found_sources:
            print(f"- {s}")